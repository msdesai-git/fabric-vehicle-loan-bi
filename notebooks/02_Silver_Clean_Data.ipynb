{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import col, count, isnan, when, mean, stddev\n","\n","# Step 1: Read raw data from Bronze\n","bronze_path = \"Files/Bronze/Loans.csv\"\n","df = spark.read.option(\"header\", True).csv(bronze_path)\n","\n","# Step 2: Drop unused column\n","df = df.drop(*[\"ProductID\", \"RiskScore\"])\n","\n","# Step 3: Rename columns & cast types\n","df = df.select(\n","    col(\"LoanID\"),\n","    col(\"CustomerID\"),\n","    col(\"BranchID\"),\n","    to_date(col(\"DisbursedDate\"), \"dd-MM-yyyy\").alias(\"StartDate\"),\n","    col(\"LoanAmount\").cast(\"double\"),\n","    col(\"TenureMonths\").cast(\"int\"),\n","    col(\"InterestRate\").cast(\"double\"),\n","    col(\"Status\"),\n","    col(\"LoanType\"),\n","    col(\"ClosureDate\").cast(\"date\"),   \n","    col(\"EarlyClosureFlag\").cast(\"boolean\")\n",")\n","\n","# Step 4: Drop rows with nulls in critical fields\n","critical_fields = [\"LoanID\", \"CustomerID\", \"LoanType\", \"LoanAmount\", \"StartDate\", \"TenureMonths\", \"BranchID\"]\n","df = df.dropna(subset=critical_fields)\n","\n","# Step 5: Remove duplicate LoanIDs (if any)\n","df = df.dropDuplicates([\"LoanID\"])\n","\n","# Step 6: Save cleaned data to Silver\n","silver_path = \"Files/Silver/Loans\"\n","df.write.mode(\"overwrite\").format(\"parquet\").save(silver_path)\n","\n","# Preview cleaned result\n","df.show(5)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":38,"statement_ids":[38],"state":"finished","livy_statement_state":"available","session_id":"f6ecba0f-a1d8-4a2d-993d-8aa10a2790ca","normalized_state":"finished","queued_time":"2025-07-01T10:58:06.1375771Z","session_start_time":null,"execution_start_time":"2025-07-01T10:58:06.1387377Z","execution_finish_time":"2025-07-01T10:58:09.6223496Z","parent_msg_id":"d9ebf513-945a-48fc-be7d-639fedbc234c"},"text/plain":"StatementMeta(, f6ecba0f-a1d8-4a2d-993d-8aa10a2790ca, 38, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+------+----------+--------+----------+----------+------------+------------+------+----------+-----------+----------------+\n|LoanID|CustomerID|BranchID| StartDate|LoanAmount|TenureMonths|InterestRate|Status|  LoanType|ClosureDate|EarlyClosureFlag|\n+------+----------+--------+----------+----------+------------+------------+------+----------+-----------+----------------+\n|L00001|    C00861|    B020|2022-10-12|1383910.76|          48|       12.49|Active|        2W|       NULL|            NULL|\n|L00002|    C01295|    B041|2020-10-14| 775199.57|          18|       14.27|Active|        2W|       NULL|            NULL|\n|L00003|    C01131|    B016|2020-12-20|1254935.95|          24|        9.52|Active|Commercial|       NULL|            NULL|\n|L00004|    C01096|    B009|2020-12-08| 510216.12|          24|        9.58|Active|        2W|       NULL|            NULL|\n|L00005|    C01639|    B030|2020-05-28|1458536.12|          18|        NULL|Active|Commercial|       NULL|            NULL|\n+------+----------+--------+----------+----------+------------+------------+------+----------+-----------+----------------+\nonly showing top 5 rows\n\n"]}],"execution_count":36,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b8c9d4ca-72b7-4bd4-9906-ea2b2810c989"},{"cell_type":"code","source":["from pyspark.sql.functions import col, to_date\n","\n","# Step 1: Read raw data from Bronze\n","bronze_path = \"Files/Bronze/EMIs.csv\"\n","df = spark.read.option(\"header\", True).csv(bronze_path)\n","\n","\n","# Step 2: Cast to appropriate data types\n","df = df.select(\n","    col(\"EMIID\"),\n","    col(\"LoanID\"),\n","    col(\"EMINumber\").cast(\"int\"),\n","    to_date(col(\"DueDate\"), \"dd-MM-yyyy\").alias(\"DueDate\"),\n","    col(\"EMIAmount\").cast(\"double\"),\n","    col(\"AmountPaid\").cast(\"double\"),\n","    to_date(col(\"PaymentDate\"), \"yyyy-MM-dd\").alias(\"PaymentDate\"),\n","    col(\"Status\")\n",")\n","\n","# Step 3: Drop nulls in critical fields\n","critical_fields = [\"EMIID\", \"LoanID\", \"DueDate\", \"EMIAmount\"]\n","df = df.dropna(subset=critical_fields)\n","\n","# Step 4: Remove duplicates\n","df = df.dropDuplicates([\"EMIID\"])\n","\n","# Step 5: Save cleaned data to Silver\n","silver_path = \"Files/Silver/EMIs\"\n","df.write.mode(\"overwrite\").format(\"parquet\").save(silver_path)\n","\n","# Preview cleaned result\n","df.show(5)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":32,"statement_ids":[32],"state":"finished","livy_statement_state":"available","session_id":"f6ecba0f-a1d8-4a2d-993d-8aa10a2790ca","normalized_state":"finished","queued_time":"2025-07-01T10:51:16.2330182Z","session_start_time":null,"execution_start_time":"2025-07-01T10:51:20.0578189Z","execution_finish_time":"2025-07-01T10:51:24.8732946Z","parent_msg_id":"36fd5d05-a1a8-4315-8682-9234fe153533"},"text/plain":"StatementMeta(, f6ecba0f-a1d8-4a2d-993d-8aa10a2790ca, 32, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---------+------+---------+----------+---------+----------+-----------+-------+\n|    EMIID|LoanID|EMINumber|   DueDate|EMIAmount|AmountPaid|PaymentDate| Status|\n+---------+------+---------+----------+---------+----------+-----------+-------+\n|E00001_05|L00001|        5|2023-03-11| 32432.53|  32432.53|       NULL|   Paid|\n|E00001_22|L00001|       22|2024-08-02| 32432.53|  32432.53|       NULL|   Paid|\n|E00001_23|L00001|       23|2024-09-01| 32432.53|  32432.53|       NULL|   Paid|\n|E00001_32|L00001|       32|2025-05-29| 32432.53|  32432.53|       NULL|   Paid|\n|E00001_42|L00001|       42|2026-03-25| 32432.53|       0.0|       NULL|Overdue|\n+---------+------+---------+----------+---------+----------+-----------+-------+\nonly showing top 5 rows\n\n"]}],"execution_count":30,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1cdd139c-3892-44de-9634-7728b7fc4f01"},{"cell_type":"code","source":["from pyspark.sql.functions import col, to_date\n","\n","# Step 1: Read raw data from Bronze\n","bronze_path = \"Files/Bronze/Customers.csv\"\n","df = spark.read.option(\"header\", True).csv(bronze_path)\n","\n","# Step 2: Drop BranchID & cast appropriate types\n","df = df.drop(\"BranchID\")\n","\n","df = df.select(\n","    col(\"CustomerID\"),\n","    col(\"CustomerName\"),\n","    col(\"Gender\"),\n","    to_date(col(\"DOB\"), \"dd-MM-yyyy\").alias(\"DOB\"),\n","    col(\"City\"),\n","    col(\"PhoneNumber\"),\n","    col(\"Email\"),\n","    col(\"PAN\"),\n","    col(\"KYCStatus\")\n",")\n","\n","# Step 3: Drop nulls in critical fields\n","critical_fields = [\"CustomerID\", \"CustomerName\", \"Gender\", \"DOB\", \"PAN\"]\n","df = df.dropna(subset=critical_fields)\n","\n","# Step 4: Remove duplicates (just in case)\n","df = df.dropDuplicates([\"CustomerID\"])\n","\n","# Step 5: Save cleaned data to Silver\n","silver_path = \"Files/Silver/Customers\"\n","df.write.mode(\"overwrite\").format(\"parquet\").save(silver_path)\n","\n","# Preview cleaned result\n","df.show(5)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":33,"statement_ids":[33],"state":"finished","livy_statement_state":"available","session_id":"f6ecba0f-a1d8-4a2d-993d-8aa10a2790ca","normalized_state":"finished","queued_time":"2025-07-01T10:51:16.4219714Z","session_start_time":null,"execution_start_time":"2025-07-01T10:51:24.8754396Z","execution_finish_time":"2025-07-01T10:51:27.2552356Z","parent_msg_id":"07a7906c-2dbd-4c1c-9aaf-3fea52598930"},"text/plain":"StatementMeta(, f6ecba0f-a1d8-4a2d-993d-8aa10a2790ca, 33, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+----------+----------------+------+----------+---------+-----------+-----------------+----------+---------+\n|CustomerID|    CustomerName|Gender|       DOB|     City|PhoneNumber|            Email|       PAN|KYCStatus|\n+----------+----------------+------+----------+---------+-----------+-----------------+----------+---------+\n|    C00001|Xhftrxc Nacgposs|Female|1994-07-06|Hyderabad|9.18808E+11|user1@example.com|WEQYI5851K| Verified|\n|    C00002|Nfpvaus Khftcjji|  Male|1982-12-25|    Delhi|9.16505E+11|user2@example.com|AJDZD0234O|  Pending|\n|    C00003|Iusnzjo Uqwpsbfh|  Male|1978-11-26|Hyderabad|9.19969E+11|user3@example.com|ZZWVP3996C| Verified|\n|    C00004|Cwwjlve Lfgyqpes|Female|1987-11-20|Hyderabad|9.17176E+11|user4@example.com|XLHFJ1828G| Verified|\n|    C00005|Fmhyrfi Yufaigfy|Female|1986-03-21|    Delhi|9.16061E+11|user5@example.com|JPZUP3947W| Verified|\n+----------+----------------+------+----------+---------+-----------+-----------------+----------+---------+\nonly showing top 5 rows\n\n"]}],"execution_count":31,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eab614b8-3cf0-4505-8751-5f68ec6db32b"},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","# Step 1: Read raw data from Bronze\n","bronze_path = \"Files/Bronze/Branches.csv\"\n","df = spark.read.option(\"header\", True).csv(bronze_path)\n","\n","# Step 2: Select and cast necessary columns\n","df = df.select(\n","    col(\"BranchID\"),\n","    col(\"BranchName\"),\n","    col(\"Region\"),\n","    col(\"Manager\"),\n","    col(\"ContactNumber\")\n",")\n","\n","# Step 3: Drop nulls in critical fields\n","critical_fields = [\"BranchID\", \"BranchName\", \"Region\"]\n","df = df.dropna(subset=critical_fields)\n","\n","# Step 4: Drop duplicates if any\n","df = df.dropDuplicates([\"BranchID\"])\n","\n","# Step 5: Save cleaned data to Silver\n","silver_path = \"Files/Silver/Branches\"\n","df.write.mode(\"overwrite\").format(\"parquet\").save(silver_path)\n","\n","# Preview cleaned result\n","df.show(5)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":34,"statement_ids":[34],"state":"finished","livy_statement_state":"available","session_id":"f6ecba0f-a1d8-4a2d-993d-8aa10a2790ca","normalized_state":"finished","queued_time":"2025-07-01T10:55:26.1133635Z","session_start_time":null,"execution_start_time":"2025-07-01T10:55:26.1144612Z","execution_finish_time":"2025-07-01T10:55:28.6741564Z","parent_msg_id":"d7e70ea6-2055-44a6-8ed9-df21fd995998"},"text/plain":"StatementMeta(, f6ecba0f-a1d8-4a2d-993d-8aa10a2790ca, 34, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+--------+-------------+------+---------------+-------------+\n|BranchID|   BranchName|Region|        Manager|ContactNumber|\n+--------+-------------+------+---------------+-------------+\n|    B001|     CredSure| North|Ecqkdvl Zpsxvms| 918588211764|\n|    B002|    LoanMetro|  East|Loziooh Hkdvewy| 916666493577|\n|    B003|ElevateCredit| North|Uwqccry Bxvqeim| 918209280975|\n|    B004| Zenith Loans|  West|Hierzub Ytrdezu| 919826998727|\n|    B005|    MoneyTree| South|Xmrnabq Hjfqizn| 918228754332|\n+--------+-------------+------+---------------+-------------+\nonly showing top 5 rows\n\n"]}],"execution_count":32,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8a092157-b0cc-4faf-b782-21052ae4768d"},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","# Step 1: Read raw data from Bronze\n","bronze_path = \"Files/Bronze/Risk_Score.csv\"\n","df = spark.read.option(\"header\", True).csv(bronze_path)\n","\n","# Step 2: Select and cast necessary columns\n","df = df.select(\n","    col(\"CustomerID\"),\n","    col(\"RiskScore\").cast(\"int\")\n",")\n","\n","# Step 3: Drop nulls in critical fields\n","df = df.dropna(subset=[\"CustomerID\", \"RiskScore\"])\n","\n","# Step 4: Drop duplicates â€“ keep latest/most frequent if needed (here we assume only 1 entry per CustomerID)\n","df = df.dropDuplicates([\"CustomerID\"])\n","\n","# Step 5: Clamp outliers if any extreme risk scores exist (e.g. >100 or <0)\n","df = df.filter((col(\"RiskScore\") >= 0) & (col(\"RiskScore\") <= 100))\n","\n","# Step 6: Save cleaned data to Silver\n","silver_path = \"Files/Silver/RiskScore\"\n","df.write.mode(\"overwrite\").format(\"parquet\").save(silver_path)\n","\n","# Preview cleaned result\n","df.show(5)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":40,"statement_ids":[40],"state":"finished","livy_statement_state":"available","session_id":"f6ecba0f-a1d8-4a2d-993d-8aa10a2790ca","normalized_state":"finished","queued_time":"2025-07-01T11:00:32.7776958Z","session_start_time":null,"execution_start_time":"2025-07-01T11:00:32.7788512Z","execution_finish_time":"2025-07-01T11:00:36.2537922Z","parent_msg_id":"205bfd0f-51aa-4391-bd58-f24585e9eb29"},"text/plain":"StatementMeta(, f6ecba0f-a1d8-4a2d-993d-8aa10a2790ca, 40, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+----------+---------+\n|CustomerID|RiskScore|\n+----------+---------+\n|    C02599|       35|\n|    C00929|       61|\n|    C01720|       65|\n|    C00126|       60|\n|    C02501|       54|\n+----------+---------+\nonly showing top 5 rows\n\n"]}],"execution_count":38,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"02b039fc-a981-41f8-9810-f0714f290ee5"},{"cell_type":"code","source":["# Silver Layer: Create ForecastData Parquet from CSV\n","\n","from pyspark.sql.functions import col\n","from pyspark.sql.types import DoubleType, IntegerType, StringType\n","\n","# Load Forecast CSV\n","df_forecast = spark.read.option(\"header\", True).csv(\"Files/Bronze/ForecastData.csv\")\n","\n","# Cast to appropriate types\n","df_forecast = df_forecast.select(\n","    col(\"LoanID\"),\n","    col(\"Forecast_OverdueAmount\").cast(DoubleType()),\n","    col(\"Forecast_NPA_Percent\").cast(DoubleType()),\n","    col(\"PredictedRiskScore\").cast(IntegerType()),\n","    col(\"PriorityRecoveryFlag\").cast(StringType())\n",")\n","\n","# Save as Parquet in Silver layer\n","df_forecast.write.mode(\"overwrite\").parquet(\"Files/Silver/ForecastData/\")\n","\n","# Preview cleaned result\n","df_forecast.show(5)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"930649ea-1c06-47c0-8f94-c23c616c8d34","normalized_state":"finished","queued_time":"2025-07-02T12:17:15.9987332Z","session_start_time":null,"execution_start_time":"2025-07-02T12:17:15.9997788Z","execution_finish_time":"2025-07-02T12:17:19.517693Z","parent_msg_id":"a4af844b-c955-448f-bc76-4cbae65ff6c6"},"text/plain":"StatementMeta(, 930649ea-1c06-47c0-8f94-c23c616c8d34, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+------+----------------------+--------------------+------------------+--------------------+\n|LoanID|Forecast_OverdueAmount|Forecast_NPA_Percent|PredictedRiskScore|PriorityRecoveryFlag|\n+------+----------------------+--------------------+------------------+--------------------+\n|L00001|     487902.9500000002|               35.26|                91|                 Yes|\n|L00002|                2011.0|                0.26|                83|                  No|\n|L00003|                2868.0|                0.23|                82|                  No|\n|L00004|                4189.0|                0.82|                64|                  No|\n|L00005|                4599.0|                0.32|              NULL|                  No|\n+------+----------------------+--------------------+------------------+--------------------+\nonly showing top 5 rows\n\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"64dddfcd-1c1c-42ab-a215-7ce0dc781525"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"a1b8cb26-d333-4af6-a4e8-7e1d0da932f0","known_lakehouses":[{"id":"a1b8cb26-d333-4af6-a4e8-7e1d0da932f0"}],"default_lakehouse_name":"VehicleLoanLakehouse","default_lakehouse_workspace_id":"7592d626-1247-49df-a5d9-8c9bde7005c1"}}},"nbformat":4,"nbformat_minor":5}